#DeepSaliency densenet+sal
name: "DeepSaliency"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "inputmap"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "/media/fangzheng/fang/code/MULtitasksal/DeepSaliency-master/dataset/trainset/hdf5/train_list.txt"
    batch_size: 2
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 3
    kernel_size: 7
    stride: 2
  }
}
layer {
  name: "conv1/bn"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv1/scale"
  type: "Scale"
  bottom: "conv1/bn"
  top: "conv1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1/bn"
  top: "conv1/bn"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1/bn"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv2_1/x1/bn"
  type: "BatchNorm"
  bottom: "pool1"
  top: "conv2_1/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv2_1/x1/scale"
  type: "Scale"
  bottom: "conv2_1/x1/bn"
  top: "conv2_1/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1/x1"
  type: "ReLU"
  bottom: "conv2_1/x1/bn"
  top: "conv2_1/x1/bn"
}
layer {
  name: "conv2_1/x1"
  type: "Convolution"
  bottom: "conv2_1/x1/bn"
  top: "conv2_1/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv2_1/x2/bn"
  type: "BatchNorm"
  bottom: "conv2_1/x1"
  top: "conv2_1/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv2_1/x2/scale"
  type: "Scale"
  bottom: "conv2_1/x2/bn"
  top: "conv2_1/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_1/x2"
  type: "ReLU"
  bottom: "conv2_1/x2/bn"
  top: "conv2_1/x2/bn"
}
layer {
  name: "conv2_1/x2"
  type: "Convolution"
  bottom: "conv2_1/x2/bn"
  top: "conv2_1/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_2_1"
  type: "Concat"
  bottom: "pool1"
  bottom: "conv2_1/x2"
  top: "concat_2_1"
}
layer {
  name: "conv2_2/x1/bn"
  type: "BatchNorm"
  bottom: "concat_2_1"
  top: "conv2_2/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv2_2/x1/scale"
  type: "Scale"
  bottom: "conv2_2/x1/bn"
  top: "conv2_2/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2/x1"
  type: "ReLU"
  bottom: "conv2_2/x1/bn"
  top: "conv2_2/x1/bn"
}
layer {
  name: "conv2_2/x1"
  type: "Convolution"
  bottom: "conv2_2/x1/bn"
  top: "conv2_2/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv2_2/x2/bn"
  type: "BatchNorm"
  bottom: "conv2_2/x1"
  top: "conv2_2/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv2_2/x2/scale"
  type: "Scale"
  bottom: "conv2_2/x2/bn"
  top: "conv2_2/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_2/x2"
  type: "ReLU"
  bottom: "conv2_2/x2/bn"
  top: "conv2_2/x2/bn"
}
layer {
  name: "conv2_2/x2"
  type: "Convolution"
  bottom: "conv2_2/x2/bn"
  top: "conv2_2/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_2_2"
  type: "Concat"
  bottom: "concat_2_1"
  bottom: "conv2_2/x2"
  top: "concat_2_2"
}
layer {
  name: "conv2_3/x1/bn"
  type: "BatchNorm"
  bottom: "concat_2_2"
  top: "conv2_3/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv2_3/x1/scale"
  type: "Scale"
  bottom: "conv2_3/x1/bn"
  top: "conv2_3/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_3/x1"
  type: "ReLU"
  bottom: "conv2_3/x1/bn"
  top: "conv2_3/x1/bn"
}
layer {
  name: "conv2_3/x1"
  type: "Convolution"
  bottom: "conv2_3/x1/bn"
  top: "conv2_3/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv2_3/x2/bn"
  type: "BatchNorm"
  bottom: "conv2_3/x1"
  top: "conv2_3/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv2_3/x2/scale"
  type: "Scale"
  bottom: "conv2_3/x2/bn"
  top: "conv2_3/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_3/x2"
  type: "ReLU"
  bottom: "conv2_3/x2/bn"
  top: "conv2_3/x2/bn"
}
layer {
  name: "conv2_3/x2"
  type: "Convolution"
  bottom: "conv2_3/x2/bn"
  top: "conv2_3/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_2_3"
  type: "Concat"
  bottom: "concat_2_2"
  bottom: "conv2_3/x2"
  top: "concat_2_3"
}
layer {
  name: "conv2_4/x1/bn"
  type: "BatchNorm"
  bottom: "concat_2_3"
  top: "conv2_4/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv2_4/x1/scale"
  type: "Scale"
  bottom: "conv2_4/x1/bn"
  top: "conv2_4/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_4/x1"
  type: "ReLU"
  bottom: "conv2_4/x1/bn"
  top: "conv2_4/x1/bn"
}
layer {
  name: "conv2_4/x1"
  type: "Convolution"
  bottom: "conv2_4/x1/bn"
  top: "conv2_4/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv2_4/x2/bn"
  type: "BatchNorm"
  bottom: "conv2_4/x1"
  top: "conv2_4/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv2_4/x2/scale"
  type: "Scale"
  bottom: "conv2_4/x2/bn"
  top: "conv2_4/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_4/x2"
  type: "ReLU"
  bottom: "conv2_4/x2/bn"
  top: "conv2_4/x2/bn"
}
layer {
  name: "conv2_4/x2"
  type: "Convolution"
  bottom: "conv2_4/x2/bn"
  top: "conv2_4/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_2_4"
  type: "Concat"
  bottom: "concat_2_3"
  bottom: "conv2_4/x2"
  top: "concat_2_4"
}
layer {
  name: "conv2_5/x1/bn"
  type: "BatchNorm"
  bottom: "concat_2_4"
  top: "conv2_5/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv2_5/x1/scale"
  type: "Scale"
  bottom: "conv2_5/x1/bn"
  top: "conv2_5/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_5/x1"
  type: "ReLU"
  bottom: "conv2_5/x1/bn"
  top: "conv2_5/x1/bn"
}
layer {
  name: "conv2_5/x1"
  type: "Convolution"
  bottom: "conv2_5/x1/bn"
  top: "conv2_5/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv2_5/x2/bn"
  type: "BatchNorm"
  bottom: "conv2_5/x1"
  top: "conv2_5/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv2_5/x2/scale"
  type: "Scale"
  bottom: "conv2_5/x2/bn"
  top: "conv2_5/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_5/x2"
  type: "ReLU"
  bottom: "conv2_5/x2/bn"
  top: "conv2_5/x2/bn"
}
layer {
  name: "conv2_5/x2"
  type: "Convolution"
  bottom: "conv2_5/x2/bn"
  top: "conv2_5/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_2_5"
  type: "Concat"
  bottom: "concat_2_4"
  bottom: "conv2_5/x2"
  top: "concat_2_5"
}
layer {
  name: "conv2_6/x1/bn"
  type: "BatchNorm"
  bottom: "concat_2_5"
  top: "conv2_6/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv2_6/x1/scale"
  type: "Scale"
  bottom: "conv2_6/x1/bn"
  top: "conv2_6/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_6/x1"
  type: "ReLU"
  bottom: "conv2_6/x1/bn"
  top: "conv2_6/x1/bn"
}
layer {
  name: "conv2_6/x1"
  type: "Convolution"
  bottom: "conv2_6/x1/bn"
  top: "conv2_6/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv2_6/x2/bn"
  type: "BatchNorm"
  bottom: "conv2_6/x1"
  top: "conv2_6/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv2_6/x2/scale"
  type: "Scale"
  bottom: "conv2_6/x2/bn"
  top: "conv2_6/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_6/x2"
  type: "ReLU"
  bottom: "conv2_6/x2/bn"
  top: "conv2_6/x2/bn"
}
layer {
  name: "conv2_6/x2"
  type: "Convolution"
  bottom: "conv2_6/x2/bn"
  top: "conv2_6/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_2_6"
  type: "Concat"
  bottom: "concat_2_5"
  bottom: "conv2_6/x2"
  top: "concat_2_6"
}
layer {
  name: "conv2_blk/bn"
  type: "BatchNorm"
  bottom: "concat_2_6"
  top: "conv2_blk/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv2_blk/scale"
  type: "Scale"
  bottom: "conv2_blk/bn"
  top: "conv2_blk/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2_blk"
  type: "ReLU"
  bottom: "conv2_blk/bn"
  top: "conv2_blk/bn"
}
layer {
  name: "conv2_blk"
  type: "Convolution"
  bottom: "conv2_blk/bn"
  top: "conv2_blk"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_blk"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1/x1/bn"
  type: "BatchNorm"
  bottom: "pool2"
  top: "conv3_1/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_1/x1/scale"
  type: "Scale"
  bottom: "conv3_1/x1/bn"
  top: "conv3_1/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1/x1"
  type: "ReLU"
  bottom: "conv3_1/x1/bn"
  top: "conv3_1/x1/bn"
}
layer {
  name: "conv3_1/x1"
  type: "Convolution"
  bottom: "conv3_1/x1/bn"
  top: "conv3_1/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_1/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_1/x1"
  top: "conv3_1/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_1/x2/scale"
  type: "Scale"
  bottom: "conv3_1/x2/bn"
  top: "conv3_1/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_1/x2"
  type: "ReLU"
  bottom: "conv3_1/x2/bn"
  top: "conv3_1/x2/bn"
}
layer {
  name: "conv3_1/x2"
  type: "Convolution"
  bottom: "conv3_1/x2/bn"
  top: "conv3_1/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_1"
  type: "Concat"
  bottom: "pool2"
  bottom: "conv3_1/x2"
  top: "concat_3_1"
}
layer {
  name: "conv3_2/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_1"
  top: "conv3_2/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_2/x1/scale"
  type: "Scale"
  bottom: "conv3_2/x1/bn"
  top: "conv3_2/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_2/x1"
  type: "ReLU"
  bottom: "conv3_2/x1/bn"
  top: "conv3_2/x1/bn"
}
layer {
  name: "conv3_2/x1"
  type: "Convolution"
  bottom: "conv3_2/x1/bn"
  top: "conv3_2/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_2/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_2/x1"
  top: "conv3_2/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_2/x2/scale"
  type: "Scale"
  bottom: "conv3_2/x2/bn"
  top: "conv3_2/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_2/x2"
  type: "ReLU"
  bottom: "conv3_2/x2/bn"
  top: "conv3_2/x2/bn"
}
layer {
  name: "conv3_2/x2"
  type: "Convolution"
  bottom: "conv3_2/x2/bn"
  top: "conv3_2/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_2"
  type: "Concat"
  bottom: "concat_3_1"
  bottom: "conv3_2/x2"
  top: "concat_3_2"
}
layer {
  name: "conv3_3/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_2"
  top: "conv3_3/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_3/x1/scale"
  type: "Scale"
  bottom: "conv3_3/x1/bn"
  top: "conv3_3/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_3/x1"
  type: "ReLU"
  bottom: "conv3_3/x1/bn"
  top: "conv3_3/x1/bn"
}
layer {
  name: "conv3_3/x1"
  type: "Convolution"
  bottom: "conv3_3/x1/bn"
  top: "conv3_3/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_3/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_3/x1"
  top: "conv3_3/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_3/x2/scale"
  type: "Scale"
  bottom: "conv3_3/x2/bn"
  top: "conv3_3/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_3/x2"
  type: "ReLU"
  bottom: "conv3_3/x2/bn"
  top: "conv3_3/x2/bn"
}
layer {
  name: "conv3_3/x2"
  type: "Convolution"
  bottom: "conv3_3/x2/bn"
  top: "conv3_3/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_3"
  type: "Concat"
  bottom: "concat_3_2"
  bottom: "conv3_3/x2"
  top: "concat_3_3"
}
layer {
  name: "conv3_4/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_3"
  top: "conv3_4/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_4/x1/scale"
  type: "Scale"
  bottom: "conv3_4/x1/bn"
  top: "conv3_4/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_4/x1"
  type: "ReLU"
  bottom: "conv3_4/x1/bn"
  top: "conv3_4/x1/bn"
}
layer {
  name: "conv3_4/x1"
  type: "Convolution"
  bottom: "conv3_4/x1/bn"
  top: "conv3_4/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_4/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_4/x1"
  top: "conv3_4/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_4/x2/scale"
  type: "Scale"
  bottom: "conv3_4/x2/bn"
  top: "conv3_4/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_4/x2"
  type: "ReLU"
  bottom: "conv3_4/x2/bn"
  top: "conv3_4/x2/bn"
}
layer {
  name: "conv3_4/x2"
  type: "Convolution"
  bottom: "conv3_4/x2/bn"
  top: "conv3_4/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_4"
  type: "Concat"
  bottom: "concat_3_3"
  bottom: "conv3_4/x2"
  top: "concat_3_4"
}
layer {
  name: "conv3_5/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_4"
  top: "conv3_5/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_5/x1/scale"
  type: "Scale"
  bottom: "conv3_5/x1/bn"
  top: "conv3_5/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_5/x1"
  type: "ReLU"
  bottom: "conv3_5/x1/bn"
  top: "conv3_5/x1/bn"
}
layer {
  name: "conv3_5/x1"
  type: "Convolution"
  bottom: "conv3_5/x1/bn"
  top: "conv3_5/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_5/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_5/x1"
  top: "conv3_5/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_5/x2/scale"
  type: "Scale"
  bottom: "conv3_5/x2/bn"
  top: "conv3_5/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_5/x2"
  type: "ReLU"
  bottom: "conv3_5/x2/bn"
  top: "conv3_5/x2/bn"
}
layer {
  name: "conv3_5/x2"
  type: "Convolution"
  bottom: "conv3_5/x2/bn"
  top: "conv3_5/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_5"
  type: "Concat"
  bottom: "concat_3_4"
  bottom: "conv3_5/x2"
  top: "concat_3_5"
}
layer {
  name: "conv3_6/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_5"
  top: "conv3_6/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_6/x1/scale"
  type: "Scale"
  bottom: "conv3_6/x1/bn"
  top: "conv3_6/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_6/x1"
  type: "ReLU"
  bottom: "conv3_6/x1/bn"
  top: "conv3_6/x1/bn"
}
layer {
  name: "conv3_6/x1"
  type: "Convolution"
  bottom: "conv3_6/x1/bn"
  top: "conv3_6/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_6/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_6/x1"
  top: "conv3_6/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_6/x2/scale"
  type: "Scale"
  bottom: "conv3_6/x2/bn"
  top: "conv3_6/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_6/x2"
  type: "ReLU"
  bottom: "conv3_6/x2/bn"
  top: "conv3_6/x2/bn"
}
layer {
  name: "conv3_6/x2"
  type: "Convolution"
  bottom: "conv3_6/x2/bn"
  top: "conv3_6/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_6"
  type: "Concat"
  bottom: "concat_3_5"
  bottom: "conv3_6/x2"
  top: "concat_3_6"
}
layer {
  name: "conv3_7/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_6"
  top: "conv3_7/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_7/x1/scale"
  type: "Scale"
  bottom: "conv3_7/x1/bn"
  top: "conv3_7/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_7/x1"
  type: "ReLU"
  bottom: "conv3_7/x1/bn"
  top: "conv3_7/x1/bn"
}
layer {
  name: "conv3_7/x1"
  type: "Convolution"
  bottom: "conv3_7/x1/bn"
  top: "conv3_7/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_7/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_7/x1"
  top: "conv3_7/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_7/x2/scale"
  type: "Scale"
  bottom: "conv3_7/x2/bn"
  top: "conv3_7/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_7/x2"
  type: "ReLU"
  bottom: "conv3_7/x2/bn"
  top: "conv3_7/x2/bn"
}
layer {
  name: "conv3_7/x2"
  type: "Convolution"
  bottom: "conv3_7/x2/bn"
  top: "conv3_7/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_7"
  type: "Concat"
  bottom: "concat_3_6"
  bottom: "conv3_7/x2"
  top: "concat_3_7"
}
layer {
  name: "conv3_8/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_7"
  top: "conv3_8/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_8/x1/scale"
  type: "Scale"
  bottom: "conv3_8/x1/bn"
  top: "conv3_8/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_8/x1"
  type: "ReLU"
  bottom: "conv3_8/x1/bn"
  top: "conv3_8/x1/bn"
}
layer {
  name: "conv3_8/x1"
  type: "Convolution"
  bottom: "conv3_8/x1/bn"
  top: "conv3_8/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_8/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_8/x1"
  top: "conv3_8/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_8/x2/scale"
  type: "Scale"
  bottom: "conv3_8/x2/bn"
  top: "conv3_8/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_8/x2"
  type: "ReLU"
  bottom: "conv3_8/x2/bn"
  top: "conv3_8/x2/bn"
}
layer {
  name: "conv3_8/x2"
  type: "Convolution"
  bottom: "conv3_8/x2/bn"
  top: "conv3_8/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_8"
  type: "Concat"
  bottom: "concat_3_7"
  bottom: "conv3_8/x2"
  top: "concat_3_8"
}
layer {
  name: "conv3_9/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_8"
  top: "conv3_9/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_9/x1/scale"
  type: "Scale"
  bottom: "conv3_9/x1/bn"
  top: "conv3_9/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_9/x1"
  type: "ReLU"
  bottom: "conv3_9/x1/bn"
  top: "conv3_9/x1/bn"
}
layer {
  name: "conv3_9/x1"
  type: "Convolution"
  bottom: "conv3_9/x1/bn"
  top: "conv3_9/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_9/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_9/x1"
  top: "conv3_9/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_9/x2/scale"
  type: "Scale"
  bottom: "conv3_9/x2/bn"
  top: "conv3_9/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_9/x2"
  type: "ReLU"
  bottom: "conv3_9/x2/bn"
  top: "conv3_9/x2/bn"
}
layer {
  name: "conv3_9/x2"
  type: "Convolution"
  bottom: "conv3_9/x2/bn"
  top: "conv3_9/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_9"
  type: "Concat"
  bottom: "concat_3_8"
  bottom: "conv3_9/x2"
  top: "concat_3_9"
}
layer {
  name: "conv3_10/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_9"
  top: "conv3_10/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_10/x1/scale"
  type: "Scale"
  bottom: "conv3_10/x1/bn"
  top: "conv3_10/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_10/x1"
  type: "ReLU"
  bottom: "conv3_10/x1/bn"
  top: "conv3_10/x1/bn"
}
layer {
  name: "conv3_10/x1"
  type: "Convolution"
  bottom: "conv3_10/x1/bn"
  top: "conv3_10/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_10/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_10/x1"
  top: "conv3_10/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_10/x2/scale"
  type: "Scale"
  bottom: "conv3_10/x2/bn"
  top: "conv3_10/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_10/x2"
  type: "ReLU"
  bottom: "conv3_10/x2/bn"
  top: "conv3_10/x2/bn"
}
layer {
  name: "conv3_10/x2"
  type: "Convolution"
  bottom: "conv3_10/x2/bn"
  top: "conv3_10/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_10"
  type: "Concat"
  bottom: "concat_3_9"
  bottom: "conv3_10/x2"
  top: "concat_3_10"
}
layer {
  name: "conv3_11/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_10"
  top: "conv3_11/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_11/x1/scale"
  type: "Scale"
  bottom: "conv3_11/x1/bn"
  top: "conv3_11/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_11/x1"
  type: "ReLU"
  bottom: "conv3_11/x1/bn"
  top: "conv3_11/x1/bn"
}
layer {
  name: "conv3_11/x1"
  type: "Convolution"
  bottom: "conv3_11/x1/bn"
  top: "conv3_11/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_11/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_11/x1"
  top: "conv3_11/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_11/x2/scale"
  type: "Scale"
  bottom: "conv3_11/x2/bn"
  top: "conv3_11/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_11/x2"
  type: "ReLU"
  bottom: "conv3_11/x2/bn"
  top: "conv3_11/x2/bn"
}
layer {
  name: "conv3_11/x2"
  type: "Convolution"
  bottom: "conv3_11/x2/bn"
  top: "conv3_11/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_11"
  type: "Concat"
  bottom: "concat_3_10"
  bottom: "conv3_11/x2"
  top: "concat_3_11"
}
layer {
  name: "conv3_12/x1/bn"
  type: "BatchNorm"
  bottom: "concat_3_11"
  top: "conv3_12/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_12/x1/scale"
  type: "Scale"
  bottom: "conv3_12/x1/bn"
  top: "conv3_12/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_12/x1"
  type: "ReLU"
  bottom: "conv3_12/x1/bn"
  top: "conv3_12/x1/bn"
}
layer {
  name: "conv3_12/x1"
  type: "Convolution"
  bottom: "conv3_12/x1/bn"
  top: "conv3_12/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv3_12/x2/bn"
  type: "BatchNorm"
  bottom: "conv3_12/x1"
  top: "conv3_12/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_12/x2/scale"
  type: "Scale"
  bottom: "conv3_12/x2/bn"
  top: "conv3_12/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_12/x2"
  type: "ReLU"
  bottom: "conv3_12/x2/bn"
  top: "conv3_12/x2/bn"
}
layer {
  name: "conv3_12/x2"
  type: "Convolution"
  bottom: "conv3_12/x2/bn"
  top: "conv3_12/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_3_12"
  type: "Concat"
  bottom: "concat_3_11"
  bottom: "conv3_12/x2"
  top: "concat_3_12"
}
layer {
  name: "conv3_blk/bn"
  type: "BatchNorm"
  bottom: "concat_3_12"
  top: "conv3_blk/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv3_blk/scale"
  type: "Scale"
  bottom: "conv3_blk/bn"
  top: "conv3_blk/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3_blk"
  type: "ReLU"
  bottom: "conv3_blk/bn"
  top: "conv3_blk/bn"
}
layer {
  name: "conv3_blk"
  type: "Convolution"
  bottom: "conv3_blk/bn"
  top: "conv3_blk"
  convolution_param {
    num_output: 384
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_blk"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1/x1/bn"
  type: "BatchNorm"
  bottom: "pool3"
  top: "conv4_1/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_1/x1/scale"
  type: "Scale"
  bottom: "conv4_1/x1/bn"
  top: "conv4_1/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1/x1"
  type: "ReLU"
  bottom: "conv4_1/x1/bn"
  top: "conv4_1/x1/bn"
}
layer {
  name: "conv4_1/x1"
  type: "Convolution"
  bottom: "conv4_1/x1/bn"
  top: "conv4_1/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_1/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_1/x1"
  top: "conv4_1/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_1/x2/scale"
  type: "Scale"
  bottom: "conv4_1/x2/bn"
  top: "conv4_1/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_1/x2"
  type: "ReLU"
  bottom: "conv4_1/x2/bn"
  top: "conv4_1/x2/bn"
}
layer {
  name: "conv4_1/x2"
  type: "Convolution"
  bottom: "conv4_1/x2/bn"
  top: "conv4_1/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_1"
  type: "Concat"
  bottom: "pool3"
  bottom: "conv4_1/x2"
  top: "concat_4_1"
}
layer {
  name: "conv4_2/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_1"
  top: "conv4_2/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_2/x1/scale"
  type: "Scale"
  bottom: "conv4_2/x1/bn"
  top: "conv4_2/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2/x1"
  type: "ReLU"
  bottom: "conv4_2/x1/bn"
  top: "conv4_2/x1/bn"
}
layer {
  name: "conv4_2/x1"
  type: "Convolution"
  bottom: "conv4_2/x1/bn"
  top: "conv4_2/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_2/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_2/x1"
  top: "conv4_2/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_2/x2/scale"
  type: "Scale"
  bottom: "conv4_2/x2/bn"
  top: "conv4_2/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_2/x2"
  type: "ReLU"
  bottom: "conv4_2/x2/bn"
  top: "conv4_2/x2/bn"
}
layer {
  name: "conv4_2/x2"
  type: "Convolution"
  bottom: "conv4_2/x2/bn"
  top: "conv4_2/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_2"
  type: "Concat"
  bottom: "concat_4_1"
  bottom: "conv4_2/x2"
  top: "concat_4_2"
}
layer {
  name: "conv4_3/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_2"
  top: "conv4_3/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_3/x1/scale"
  type: "Scale"
  bottom: "conv4_3/x1/bn"
  top: "conv4_3/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_3/x1"
  type: "ReLU"
  bottom: "conv4_3/x1/bn"
  top: "conv4_3/x1/bn"
}
layer {
  name: "conv4_3/x1"
  type: "Convolution"
  bottom: "conv4_3/x1/bn"
  top: "conv4_3/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_3/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_3/x1"
  top: "conv4_3/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_3/x2/scale"
  type: "Scale"
  bottom: "conv4_3/x2/bn"
  top: "conv4_3/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_3/x2"
  type: "ReLU"
  bottom: "conv4_3/x2/bn"
  top: "conv4_3/x2/bn"
}
layer {
  name: "conv4_3/x2"
  type: "Convolution"
  bottom: "conv4_3/x2/bn"
  top: "conv4_3/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_3"
  type: "Concat"
  bottom: "concat_4_2"
  bottom: "conv4_3/x2"
  top: "concat_4_3"
}
layer {
  name: "conv4_4/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_3"
  top: "conv4_4/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_4/x1/scale"
  type: "Scale"
  bottom: "conv4_4/x1/bn"
  top: "conv4_4/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_4/x1"
  type: "ReLU"
  bottom: "conv4_4/x1/bn"
  top: "conv4_4/x1/bn"
}
layer {
  name: "conv4_4/x1"
  type: "Convolution"
  bottom: "conv4_4/x1/bn"
  top: "conv4_4/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_4/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_4/x1"
  top: "conv4_4/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_4/x2/scale"
  type: "Scale"
  bottom: "conv4_4/x2/bn"
  top: "conv4_4/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_4/x2"
  type: "ReLU"
  bottom: "conv4_4/x2/bn"
  top: "conv4_4/x2/bn"
}
layer {
  name: "conv4_4/x2"
  type: "Convolution"
  bottom: "conv4_4/x2/bn"
  top: "conv4_4/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_4"
  type: "Concat"
  bottom: "concat_4_3"
  bottom: "conv4_4/x2"
  top: "concat_4_4"
}
layer {
  name: "conv4_5/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_4"
  top: "conv4_5/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_5/x1/scale"
  type: "Scale"
  bottom: "conv4_5/x1/bn"
  top: "conv4_5/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_5/x1"
  type: "ReLU"
  bottom: "conv4_5/x1/bn"
  top: "conv4_5/x1/bn"
}
layer {
  name: "conv4_5/x1"
  type: "Convolution"
  bottom: "conv4_5/x1/bn"
  top: "conv4_5/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_5/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_5/x1"
  top: "conv4_5/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_5/x2/scale"
  type: "Scale"
  bottom: "conv4_5/x2/bn"
  top: "conv4_5/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_5/x2"
  type: "ReLU"
  bottom: "conv4_5/x2/bn"
  top: "conv4_5/x2/bn"
}
layer {
  name: "conv4_5/x2"
  type: "Convolution"
  bottom: "conv4_5/x2/bn"
  top: "conv4_5/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_5"
  type: "Concat"
  bottom: "concat_4_4"
  bottom: "conv4_5/x2"
  top: "concat_4_5"
}
layer {
  name: "conv4_6/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_5"
  top: "conv4_6/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_6/x1/scale"
  type: "Scale"
  bottom: "conv4_6/x1/bn"
  top: "conv4_6/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_6/x1"
  type: "ReLU"
  bottom: "conv4_6/x1/bn"
  top: "conv4_6/x1/bn"
}
layer {
  name: "conv4_6/x1"
  type: "Convolution"
  bottom: "conv4_6/x1/bn"
  top: "conv4_6/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_6/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_6/x1"
  top: "conv4_6/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_6/x2/scale"
  type: "Scale"
  bottom: "conv4_6/x2/bn"
  top: "conv4_6/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_6/x2"
  type: "ReLU"
  bottom: "conv4_6/x2/bn"
  top: "conv4_6/x2/bn"
}
layer {
  name: "conv4_6/x2"
  type: "Convolution"
  bottom: "conv4_6/x2/bn"
  top: "conv4_6/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_6"
  type: "Concat"
  bottom: "concat_4_5"
  bottom: "conv4_6/x2"
  top: "concat_4_6"
}
layer {
  name: "conv4_7/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_6"
  top: "conv4_7/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_7/x1/scale"
  type: "Scale"
  bottom: "conv4_7/x1/bn"
  top: "conv4_7/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_7/x1"
  type: "ReLU"
  bottom: "conv4_7/x1/bn"
  top: "conv4_7/x1/bn"
}
layer {
  name: "conv4_7/x1"
  type: "Convolution"
  bottom: "conv4_7/x1/bn"
  top: "conv4_7/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_7/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_7/x1"
  top: "conv4_7/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_7/x2/scale"
  type: "Scale"
  bottom: "conv4_7/x2/bn"
  top: "conv4_7/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_7/x2"
  type: "ReLU"
  bottom: "conv4_7/x2/bn"
  top: "conv4_7/x2/bn"
}
layer {
  name: "conv4_7/x2"
  type: "Convolution"
  bottom: "conv4_7/x2/bn"
  top: "conv4_7/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_7"
  type: "Concat"
  bottom: "concat_4_6"
  bottom: "conv4_7/x2"
  top: "concat_4_7"
}
layer {
  name: "conv4_8/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_7"
  top: "conv4_8/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_8/x1/scale"
  type: "Scale"
  bottom: "conv4_8/x1/bn"
  top: "conv4_8/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_8/x1"
  type: "ReLU"
  bottom: "conv4_8/x1/bn"
  top: "conv4_8/x1/bn"
}
layer {
  name: "conv4_8/x1"
  type: "Convolution"
  bottom: "conv4_8/x1/bn"
  top: "conv4_8/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_8/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_8/x1"
  top: "conv4_8/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_8/x2/scale"
  type: "Scale"
  bottom: "conv4_8/x2/bn"
  top: "conv4_8/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_8/x2"
  type: "ReLU"
  bottom: "conv4_8/x2/bn"
  top: "conv4_8/x2/bn"
}
layer {
  name: "conv4_8/x2"
  type: "Convolution"
  bottom: "conv4_8/x2/bn"
  top: "conv4_8/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_8"
  type: "Concat"
  bottom: "concat_4_7"
  bottom: "conv4_8/x2"
  top: "concat_4_8"
}
layer {
  name: "conv4_9/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_8"
  top: "conv4_9/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_9/x1/scale"
  type: "Scale"
  bottom: "conv4_9/x1/bn"
  top: "conv4_9/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_9/x1"
  type: "ReLU"
  bottom: "conv4_9/x1/bn"
  top: "conv4_9/x1/bn"
}
layer {
  name: "conv4_9/x1"
  type: "Convolution"
  bottom: "conv4_9/x1/bn"
  top: "conv4_9/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_9/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_9/x1"
  top: "conv4_9/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_9/x2/scale"
  type: "Scale"
  bottom: "conv4_9/x2/bn"
  top: "conv4_9/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_9/x2"
  type: "ReLU"
  bottom: "conv4_9/x2/bn"
  top: "conv4_9/x2/bn"
}
layer {
  name: "conv4_9/x2"
  type: "Convolution"
  bottom: "conv4_9/x2/bn"
  top: "conv4_9/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_9"
  type: "Concat"
  bottom: "concat_4_8"
  bottom: "conv4_9/x2"
  top: "concat_4_9"
}
layer {
  name: "conv4_10/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_9"
  top: "conv4_10/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_10/x1/scale"
  type: "Scale"
  bottom: "conv4_10/x1/bn"
  top: "conv4_10/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_10/x1"
  type: "ReLU"
  bottom: "conv4_10/x1/bn"
  top: "conv4_10/x1/bn"
}
layer {
  name: "conv4_10/x1"
  type: "Convolution"
  bottom: "conv4_10/x1/bn"
  top: "conv4_10/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_10/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_10/x1"
  top: "conv4_10/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_10/x2/scale"
  type: "Scale"
  bottom: "conv4_10/x2/bn"
  top: "conv4_10/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_10/x2"
  type: "ReLU"
  bottom: "conv4_10/x2/bn"
  top: "conv4_10/x2/bn"
}
layer {
  name: "conv4_10/x2"
  type: "Convolution"
  bottom: "conv4_10/x2/bn"
  top: "conv4_10/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_10"
  type: "Concat"
  bottom: "concat_4_9"
  bottom: "conv4_10/x2"
  top: "concat_4_10"
}
layer {
  name: "conv4_11/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_10"
  top: "conv4_11/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_11/x1/scale"
  type: "Scale"
  bottom: "conv4_11/x1/bn"
  top: "conv4_11/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_11/x1"
  type: "ReLU"
  bottom: "conv4_11/x1/bn"
  top: "conv4_11/x1/bn"
}
layer {
  name: "conv4_11/x1"
  type: "Convolution"
  bottom: "conv4_11/x1/bn"
  top: "conv4_11/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_11/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_11/x1"
  top: "conv4_11/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_11/x2/scale"
  type: "Scale"
  bottom: "conv4_11/x2/bn"
  top: "conv4_11/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_11/x2"
  type: "ReLU"
  bottom: "conv4_11/x2/bn"
  top: "conv4_11/x2/bn"
}
layer {
  name: "conv4_11/x2"
  type: "Convolution"
  bottom: "conv4_11/x2/bn"
  top: "conv4_11/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_11"
  type: "Concat"
  bottom: "concat_4_10"
  bottom: "conv4_11/x2"
  top: "concat_4_11"
}
layer {
  name: "conv4_12/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_11"
  top: "conv4_12/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_12/x1/scale"
  type: "Scale"
  bottom: "conv4_12/x1/bn"
  top: "conv4_12/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_12/x1"
  type: "ReLU"
  bottom: "conv4_12/x1/bn"
  top: "conv4_12/x1/bn"
}
layer {
  name: "conv4_12/x1"
  type: "Convolution"
  bottom: "conv4_12/x1/bn"
  top: "conv4_12/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_12/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_12/x1"
  top: "conv4_12/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_12/x2/scale"
  type: "Scale"
  bottom: "conv4_12/x2/bn"
  top: "conv4_12/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_12/x2"
  type: "ReLU"
  bottom: "conv4_12/x2/bn"
  top: "conv4_12/x2/bn"
}
layer {
  name: "conv4_12/x2"
  type: "Convolution"
  bottom: "conv4_12/x2/bn"
  top: "conv4_12/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_12"
  type: "Concat"
  bottom: "concat_4_11"
  bottom: "conv4_12/x2"
  top: "concat_4_12"
}
layer {
  name: "conv4_13/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_12"
  top: "conv4_13/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_13/x1/scale"
  type: "Scale"
  bottom: "conv4_13/x1/bn"
  top: "conv4_13/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_13/x1"
  type: "ReLU"
  bottom: "conv4_13/x1/bn"
  top: "conv4_13/x1/bn"
}
layer {
  name: "conv4_13/x1"
  type: "Convolution"
  bottom: "conv4_13/x1/bn"
  top: "conv4_13/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_13/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_13/x1"
  top: "conv4_13/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_13/x2/scale"
  type: "Scale"
  bottom: "conv4_13/x2/bn"
  top: "conv4_13/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_13/x2"
  type: "ReLU"
  bottom: "conv4_13/x2/bn"
  top: "conv4_13/x2/bn"
}
layer {
  name: "conv4_13/x2"
  type: "Convolution"
  bottom: "conv4_13/x2/bn"
  top: "conv4_13/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_13"
  type: "Concat"
  bottom: "concat_4_12"
  bottom: "conv4_13/x2"
  top: "concat_4_13"
}
layer {
  name: "conv4_14/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_13"
  top: "conv4_14/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_14/x1/scale"
  type: "Scale"
  bottom: "conv4_14/x1/bn"
  top: "conv4_14/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_14/x1"
  type: "ReLU"
  bottom: "conv4_14/x1/bn"
  top: "conv4_14/x1/bn"
}
layer {
  name: "conv4_14/x1"
  type: "Convolution"
  bottom: "conv4_14/x1/bn"
  top: "conv4_14/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_14/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_14/x1"
  top: "conv4_14/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_14/x2/scale"
  type: "Scale"
  bottom: "conv4_14/x2/bn"
  top: "conv4_14/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_14/x2"
  type: "ReLU"
  bottom: "conv4_14/x2/bn"
  top: "conv4_14/x2/bn"
}
layer {
  name: "conv4_14/x2"
  type: "Convolution"
  bottom: "conv4_14/x2/bn"
  top: "conv4_14/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_14"
  type: "Concat"
  bottom: "concat_4_13"
  bottom: "conv4_14/x2"
  top: "concat_4_14"
}
layer {
  name: "conv4_15/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_14"
  top: "conv4_15/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_15/x1/scale"
  type: "Scale"
  bottom: "conv4_15/x1/bn"
  top: "conv4_15/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_15/x1"
  type: "ReLU"
  bottom: "conv4_15/x1/bn"
  top: "conv4_15/x1/bn"
}
layer {
  name: "conv4_15/x1"
  type: "Convolution"
  bottom: "conv4_15/x1/bn"
  top: "conv4_15/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_15/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_15/x1"
  top: "conv4_15/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_15/x2/scale"
  type: "Scale"
  bottom: "conv4_15/x2/bn"
  top: "conv4_15/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_15/x2"
  type: "ReLU"
  bottom: "conv4_15/x2/bn"
  top: "conv4_15/x2/bn"
}
layer {
  name: "conv4_15/x2"
  type: "Convolution"
  bottom: "conv4_15/x2/bn"
  top: "conv4_15/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_15"
  type: "Concat"
  bottom: "concat_4_14"
  bottom: "conv4_15/x2"
  top: "concat_4_15"
}
layer {
  name: "conv4_16/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_15"
  top: "conv4_16/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_16/x1/scale"
  type: "Scale"
  bottom: "conv4_16/x1/bn"
  top: "conv4_16/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_16/x1"
  type: "ReLU"
  bottom: "conv4_16/x1/bn"
  top: "conv4_16/x1/bn"
}
layer {
  name: "conv4_16/x1"
  type: "Convolution"
  bottom: "conv4_16/x1/bn"
  top: "conv4_16/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_16/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_16/x1"
  top: "conv4_16/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_16/x2/scale"
  type: "Scale"
  bottom: "conv4_16/x2/bn"
  top: "conv4_16/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_16/x2"
  type: "ReLU"
  bottom: "conv4_16/x2/bn"
  top: "conv4_16/x2/bn"
}
layer {
  name: "conv4_16/x2"
  type: "Convolution"
  bottom: "conv4_16/x2/bn"
  top: "conv4_16/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_16"
  type: "Concat"
  bottom: "concat_4_15"
  bottom: "conv4_16/x2"
  top: "concat_4_16"
}
layer {
  name: "conv4_17/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_16"
  top: "conv4_17/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_17/x1/scale"
  type: "Scale"
  bottom: "conv4_17/x1/bn"
  top: "conv4_17/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_17/x1"
  type: "ReLU"
  bottom: "conv4_17/x1/bn"
  top: "conv4_17/x1/bn"
}
layer {
  name: "conv4_17/x1"
  type: "Convolution"
  bottom: "conv4_17/x1/bn"
  top: "conv4_17/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_17/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_17/x1"
  top: "conv4_17/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_17/x2/scale"
  type: "Scale"
  bottom: "conv4_17/x2/bn"
  top: "conv4_17/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_17/x2"
  type: "ReLU"
  bottom: "conv4_17/x2/bn"
  top: "conv4_17/x2/bn"
}
layer {
  name: "conv4_17/x2"
  type: "Convolution"
  bottom: "conv4_17/x2/bn"
  top: "conv4_17/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_17"
  type: "Concat"
  bottom: "concat_4_16"
  bottom: "conv4_17/x2"
  top: "concat_4_17"
}
layer {
  name: "conv4_18/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_17"
  top: "conv4_18/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_18/x1/scale"
  type: "Scale"
  bottom: "conv4_18/x1/bn"
  top: "conv4_18/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_18/x1"
  type: "ReLU"
  bottom: "conv4_18/x1/bn"
  top: "conv4_18/x1/bn"
}
layer {
  name: "conv4_18/x1"
  type: "Convolution"
  bottom: "conv4_18/x1/bn"
  top: "conv4_18/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_18/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_18/x1"
  top: "conv4_18/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_18/x2/scale"
  type: "Scale"
  bottom: "conv4_18/x2/bn"
  top: "conv4_18/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_18/x2"
  type: "ReLU"
  bottom: "conv4_18/x2/bn"
  top: "conv4_18/x2/bn"
}
layer {
  name: "conv4_18/x2"
  type: "Convolution"
  bottom: "conv4_18/x2/bn"
  top: "conv4_18/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_18"
  type: "Concat"
  bottom: "concat_4_17"
  bottom: "conv4_18/x2"
  top: "concat_4_18"
}
layer {
  name: "conv4_19/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_18"
  top: "conv4_19/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_19/x1/scale"
  type: "Scale"
  bottom: "conv4_19/x1/bn"
  top: "conv4_19/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_19/x1"
  type: "ReLU"
  bottom: "conv4_19/x1/bn"
  top: "conv4_19/x1/bn"
}
layer {
  name: "conv4_19/x1"
  type: "Convolution"
  bottom: "conv4_19/x1/bn"
  top: "conv4_19/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_19/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_19/x1"
  top: "conv4_19/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_19/x2/scale"
  type: "Scale"
  bottom: "conv4_19/x2/bn"
  top: "conv4_19/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_19/x2"
  type: "ReLU"
  bottom: "conv4_19/x2/bn"
  top: "conv4_19/x2/bn"
}
layer {
  name: "conv4_19/x2"
  type: "Convolution"
  bottom: "conv4_19/x2/bn"
  top: "conv4_19/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_19"
  type: "Concat"
  bottom: "concat_4_18"
  bottom: "conv4_19/x2"
  top: "concat_4_19"
}
layer {
  name: "conv4_20/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_19"
  top: "conv4_20/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_20/x1/scale"
  type: "Scale"
  bottom: "conv4_20/x1/bn"
  top: "conv4_20/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_20/x1"
  type: "ReLU"
  bottom: "conv4_20/x1/bn"
  top: "conv4_20/x1/bn"
}
layer {
  name: "conv4_20/x1"
  type: "Convolution"
  bottom: "conv4_20/x1/bn"
  top: "conv4_20/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_20/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_20/x1"
  top: "conv4_20/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_20/x2/scale"
  type: "Scale"
  bottom: "conv4_20/x2/bn"
  top: "conv4_20/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_20/x2"
  type: "ReLU"
  bottom: "conv4_20/x2/bn"
  top: "conv4_20/x2/bn"
}
layer {
  name: "conv4_20/x2"
  type: "Convolution"
  bottom: "conv4_20/x2/bn"
  top: "conv4_20/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_20"
  type: "Concat"
  bottom: "concat_4_19"
  bottom: "conv4_20/x2"
  top: "concat_4_20"
}
layer {
  name: "conv4_21/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_20"
  top: "conv4_21/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_21/x1/scale"
  type: "Scale"
  bottom: "conv4_21/x1/bn"
  top: "conv4_21/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_21/x1"
  type: "ReLU"
  bottom: "conv4_21/x1/bn"
  top: "conv4_21/x1/bn"
}
layer {
  name: "conv4_21/x1"
  type: "Convolution"
  bottom: "conv4_21/x1/bn"
  top: "conv4_21/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_21/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_21/x1"
  top: "conv4_21/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_21/x2/scale"
  type: "Scale"
  bottom: "conv4_21/x2/bn"
  top: "conv4_21/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_21/x2"
  type: "ReLU"
  bottom: "conv4_21/x2/bn"
  top: "conv4_21/x2/bn"
}
layer {
  name: "conv4_21/x2"
  type: "Convolution"
  bottom: "conv4_21/x2/bn"
  top: "conv4_21/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_21"
  type: "Concat"
  bottom: "concat_4_20"
  bottom: "conv4_21/x2"
  top: "concat_4_21"
}
layer {
  name: "conv4_22/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_21"
  top: "conv4_22/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_22/x1/scale"
  type: "Scale"
  bottom: "conv4_22/x1/bn"
  top: "conv4_22/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_22/x1"
  type: "ReLU"
  bottom: "conv4_22/x1/bn"
  top: "conv4_22/x1/bn"
}
layer {
  name: "conv4_22/x1"
  type: "Convolution"
  bottom: "conv4_22/x1/bn"
  top: "conv4_22/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_22/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_22/x1"
  top: "conv4_22/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_22/x2/scale"
  type: "Scale"
  bottom: "conv4_22/x2/bn"
  top: "conv4_22/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_22/x2"
  type: "ReLU"
  bottom: "conv4_22/x2/bn"
  top: "conv4_22/x2/bn"
}
layer {
  name: "conv4_22/x2"
  type: "Convolution"
  bottom: "conv4_22/x2/bn"
  top: "conv4_22/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_22"
  type: "Concat"
  bottom: "concat_4_21"
  bottom: "conv4_22/x2"
  top: "concat_4_22"
}
layer {
  name: "conv4_23/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_22"
  top: "conv4_23/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_23/x1/scale"
  type: "Scale"
  bottom: "conv4_23/x1/bn"
  top: "conv4_23/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_23/x1"
  type: "ReLU"
  bottom: "conv4_23/x1/bn"
  top: "conv4_23/x1/bn"
}
layer {
  name: "conv4_23/x1"
  type: "Convolution"
  bottom: "conv4_23/x1/bn"
  top: "conv4_23/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_23/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_23/x1"
  top: "conv4_23/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_23/x2/scale"
  type: "Scale"
  bottom: "conv4_23/x2/bn"
  top: "conv4_23/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_23/x2"
  type: "ReLU"
  bottom: "conv4_23/x2/bn"
  top: "conv4_23/x2/bn"
}
layer {
  name: "conv4_23/x2"
  type: "Convolution"
  bottom: "conv4_23/x2/bn"
  top: "conv4_23/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_23"
  type: "Concat"
  bottom: "concat_4_22"
  bottom: "conv4_23/x2"
  top: "concat_4_23"
}
layer {
  name: "conv4_24/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_23"
  top: "conv4_24/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_24/x1/scale"
  type: "Scale"
  bottom: "conv4_24/x1/bn"
  top: "conv4_24/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_24/x1"
  type: "ReLU"
  bottom: "conv4_24/x1/bn"
  top: "conv4_24/x1/bn"
}
layer {
  name: "conv4_24/x1"
  type: "Convolution"
  bottom: "conv4_24/x1/bn"
  top: "conv4_24/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_24/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_24/x1"
  top: "conv4_24/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_24/x2/scale"
  type: "Scale"
  bottom: "conv4_24/x2/bn"
  top: "conv4_24/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_24/x2"
  type: "ReLU"
  bottom: "conv4_24/x2/bn"
  top: "conv4_24/x2/bn"
}
layer {
  name: "conv4_24/x2"
  type: "Convolution"
  bottom: "conv4_24/x2/bn"
  top: "conv4_24/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_24"
  type: "Concat"
  bottom: "concat_4_23"
  bottom: "conv4_24/x2"
  top: "concat_4_24"
}
layer {
  name: "conv4_25/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_24"
  top: "conv4_25/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_25/x1/scale"
  type: "Scale"
  bottom: "conv4_25/x1/bn"
  top: "conv4_25/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_25/x1"
  type: "ReLU"
  bottom: "conv4_25/x1/bn"
  top: "conv4_25/x1/bn"
}
layer {
  name: "conv4_25/x1"
  type: "Convolution"
  bottom: "conv4_25/x1/bn"
  top: "conv4_25/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_25/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_25/x1"
  top: "conv4_25/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_25/x2/scale"
  type: "Scale"
  bottom: "conv4_25/x2/bn"
  top: "conv4_25/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_25/x2"
  type: "ReLU"
  bottom: "conv4_25/x2/bn"
  top: "conv4_25/x2/bn"
}
layer {
  name: "conv4_25/x2"
  type: "Convolution"
  bottom: "conv4_25/x2/bn"
  top: "conv4_25/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_25"
  type: "Concat"
  bottom: "concat_4_24"
  bottom: "conv4_25/x2"
  top: "concat_4_25"
}
layer {
  name: "conv4_26/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_25"
  top: "conv4_26/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_26/x1/scale"
  type: "Scale"
  bottom: "conv4_26/x1/bn"
  top: "conv4_26/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_26/x1"
  type: "ReLU"
  bottom: "conv4_26/x1/bn"
  top: "conv4_26/x1/bn"
}
layer {
  name: "conv4_26/x1"
  type: "Convolution"
  bottom: "conv4_26/x1/bn"
  top: "conv4_26/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_26/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_26/x1"
  top: "conv4_26/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_26/x2/scale"
  type: "Scale"
  bottom: "conv4_26/x2/bn"
  top: "conv4_26/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_26/x2"
  type: "ReLU"
  bottom: "conv4_26/x2/bn"
  top: "conv4_26/x2/bn"
}
layer {
  name: "conv4_26/x2"
  type: "Convolution"
  bottom: "conv4_26/x2/bn"
  top: "conv4_26/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_26"
  type: "Concat"
  bottom: "concat_4_25"
  bottom: "conv4_26/x2"
  top: "concat_4_26"
}
layer {
  name: "conv4_27/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_26"
  top: "conv4_27/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_27/x1/scale"
  type: "Scale"
  bottom: "conv4_27/x1/bn"
  top: "conv4_27/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_27/x1"
  type: "ReLU"
  bottom: "conv4_27/x1/bn"
  top: "conv4_27/x1/bn"
}
layer {
  name: "conv4_27/x1"
  type: "Convolution"
  bottom: "conv4_27/x1/bn"
  top: "conv4_27/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_27/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_27/x1"
  top: "conv4_27/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_27/x2/scale"
  type: "Scale"
  bottom: "conv4_27/x2/bn"
  top: "conv4_27/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_27/x2"
  type: "ReLU"
  bottom: "conv4_27/x2/bn"
  top: "conv4_27/x2/bn"
}
layer {
  name: "conv4_27/x2"
  type: "Convolution"
  bottom: "conv4_27/x2/bn"
  top: "conv4_27/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_27"
  type: "Concat"
  bottom: "concat_4_26"
  bottom: "conv4_27/x2"
  top: "concat_4_27"
}
layer {
  name: "conv4_28/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_27"
  top: "conv4_28/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_28/x1/scale"
  type: "Scale"
  bottom: "conv4_28/x1/bn"
  top: "conv4_28/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_28/x1"
  type: "ReLU"
  bottom: "conv4_28/x1/bn"
  top: "conv4_28/x1/bn"
}
layer {
  name: "conv4_28/x1"
  type: "Convolution"
  bottom: "conv4_28/x1/bn"
  top: "conv4_28/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_28/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_28/x1"
  top: "conv4_28/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_28/x2/scale"
  type: "Scale"
  bottom: "conv4_28/x2/bn"
  top: "conv4_28/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_28/x2"
  type: "ReLU"
  bottom: "conv4_28/x2/bn"
  top: "conv4_28/x2/bn"
}
layer {
  name: "conv4_28/x2"
  type: "Convolution"
  bottom: "conv4_28/x2/bn"
  top: "conv4_28/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_28"
  type: "Concat"
  bottom: "concat_4_27"
  bottom: "conv4_28/x2"
  top: "concat_4_28"
}
layer {
  name: "conv4_29/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_28"
  top: "conv4_29/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_29/x1/scale"
  type: "Scale"
  bottom: "conv4_29/x1/bn"
  top: "conv4_29/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_29/x1"
  type: "ReLU"
  bottom: "conv4_29/x1/bn"
  top: "conv4_29/x1/bn"
}
layer {
  name: "conv4_29/x1"
  type: "Convolution"
  bottom: "conv4_29/x1/bn"
  top: "conv4_29/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_29/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_29/x1"
  top: "conv4_29/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_29/x2/scale"
  type: "Scale"
  bottom: "conv4_29/x2/bn"
  top: "conv4_29/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_29/x2"
  type: "ReLU"
  bottom: "conv4_29/x2/bn"
  top: "conv4_29/x2/bn"
}
layer {
  name: "conv4_29/x2"
  type: "Convolution"
  bottom: "conv4_29/x2/bn"
  top: "conv4_29/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_29"
  type: "Concat"
  bottom: "concat_4_28"
  bottom: "conv4_29/x2"
  top: "concat_4_29"
}
layer {
  name: "conv4_30/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_29"
  top: "conv4_30/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_30/x1/scale"
  type: "Scale"
  bottom: "conv4_30/x1/bn"
  top: "conv4_30/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_30/x1"
  type: "ReLU"
  bottom: "conv4_30/x1/bn"
  top: "conv4_30/x1/bn"
}
layer {
  name: "conv4_30/x1"
  type: "Convolution"
  bottom: "conv4_30/x1/bn"
  top: "conv4_30/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_30/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_30/x1"
  top: "conv4_30/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_30/x2/scale"
  type: "Scale"
  bottom: "conv4_30/x2/bn"
  top: "conv4_30/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_30/x2"
  type: "ReLU"
  bottom: "conv4_30/x2/bn"
  top: "conv4_30/x2/bn"
}
layer {
  name: "conv4_30/x2"
  type: "Convolution"
  bottom: "conv4_30/x2/bn"
  top: "conv4_30/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_30"
  type: "Concat"
  bottom: "concat_4_29"
  bottom: "conv4_30/x2"
  top: "concat_4_30"
}
layer {
  name: "conv4_31/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_30"
  top: "conv4_31/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_31/x1/scale"
  type: "Scale"
  bottom: "conv4_31/x1/bn"
  top: "conv4_31/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_31/x1"
  type: "ReLU"
  bottom: "conv4_31/x1/bn"
  top: "conv4_31/x1/bn"
}
layer {
  name: "conv4_31/x1"
  type: "Convolution"
  bottom: "conv4_31/x1/bn"
  top: "conv4_31/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_31/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_31/x1"
  top: "conv4_31/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_31/x2/scale"
  type: "Scale"
  bottom: "conv4_31/x2/bn"
  top: "conv4_31/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_31/x2"
  type: "ReLU"
  bottom: "conv4_31/x2/bn"
  top: "conv4_31/x2/bn"
}
layer {
  name: "conv4_31/x2"
  type: "Convolution"
  bottom: "conv4_31/x2/bn"
  top: "conv4_31/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_31"
  type: "Concat"
  bottom: "concat_4_30"
  bottom: "conv4_31/x2"
  top: "concat_4_31"
}
layer {
  name: "conv4_32/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_31"
  top: "conv4_32/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_32/x1/scale"
  type: "Scale"
  bottom: "conv4_32/x1/bn"
  top: "conv4_32/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_32/x1"
  type: "ReLU"
  bottom: "conv4_32/x1/bn"
  top: "conv4_32/x1/bn"
}
layer {
  name: "conv4_32/x1"
  type: "Convolution"
  bottom: "conv4_32/x1/bn"
  top: "conv4_32/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_32/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_32/x1"
  top: "conv4_32/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_32/x2/scale"
  type: "Scale"
  bottom: "conv4_32/x2/bn"
  top: "conv4_32/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_32/x2"
  type: "ReLU"
  bottom: "conv4_32/x2/bn"
  top: "conv4_32/x2/bn"
}
layer {
  name: "conv4_32/x2"
  type: "Convolution"
  bottom: "conv4_32/x2/bn"
  top: "conv4_32/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_32"
  type: "Concat"
  bottom: "concat_4_31"
  bottom: "conv4_32/x2"
  top: "concat_4_32"
}
layer {
  name: "conv4_33/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_32"
  top: "conv4_33/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_33/x1/scale"
  type: "Scale"
  bottom: "conv4_33/x1/bn"
  top: "conv4_33/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_33/x1"
  type: "ReLU"
  bottom: "conv4_33/x1/bn"
  top: "conv4_33/x1/bn"
}
layer {
  name: "conv4_33/x1"
  type: "Convolution"
  bottom: "conv4_33/x1/bn"
  top: "conv4_33/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_33/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_33/x1"
  top: "conv4_33/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_33/x2/scale"
  type: "Scale"
  bottom: "conv4_33/x2/bn"
  top: "conv4_33/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_33/x2"
  type: "ReLU"
  bottom: "conv4_33/x2/bn"
  top: "conv4_33/x2/bn"
}
layer {
  name: "conv4_33/x2"
  type: "Convolution"
  bottom: "conv4_33/x2/bn"
  top: "conv4_33/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_33"
  type: "Concat"
  bottom: "concat_4_32"
  bottom: "conv4_33/x2"
  top: "concat_4_33"
}
layer {
  name: "conv4_34/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_33"
  top: "conv4_34/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_34/x1/scale"
  type: "Scale"
  bottom: "conv4_34/x1/bn"
  top: "conv4_34/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_34/x1"
  type: "ReLU"
  bottom: "conv4_34/x1/bn"
  top: "conv4_34/x1/bn"
}
layer {
  name: "conv4_34/x1"
  type: "Convolution"
  bottom: "conv4_34/x1/bn"
  top: "conv4_34/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_34/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_34/x1"
  top: "conv4_34/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_34/x2/scale"
  type: "Scale"
  bottom: "conv4_34/x2/bn"
  top: "conv4_34/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_34/x2"
  type: "ReLU"
  bottom: "conv4_34/x2/bn"
  top: "conv4_34/x2/bn"
}
layer {
  name: "conv4_34/x2"
  type: "Convolution"
  bottom: "conv4_34/x2/bn"
  top: "conv4_34/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_34"
  type: "Concat"
  bottom: "concat_4_33"
  bottom: "conv4_34/x2"
  top: "concat_4_34"
}
layer {
  name: "conv4_35/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_34"
  top: "conv4_35/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_35/x1/scale"
  type: "Scale"
  bottom: "conv4_35/x1/bn"
  top: "conv4_35/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_35/x1"
  type: "ReLU"
  bottom: "conv4_35/x1/bn"
  top: "conv4_35/x1/bn"
}
layer {
  name: "conv4_35/x1"
  type: "Convolution"
  bottom: "conv4_35/x1/bn"
  top: "conv4_35/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_35/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_35/x1"
  top: "conv4_35/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_35/x2/scale"
  type: "Scale"
  bottom: "conv4_35/x2/bn"
  top: "conv4_35/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_35/x2"
  type: "ReLU"
  bottom: "conv4_35/x2/bn"
  top: "conv4_35/x2/bn"
}
layer {
  name: "conv4_35/x2"
  type: "Convolution"
  bottom: "conv4_35/x2/bn"
  top: "conv4_35/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_35"
  type: "Concat"
  bottom: "concat_4_34"
  bottom: "conv4_35/x2"
  top: "concat_4_35"
}
layer {
  name: "conv4_36/x1/bn"
  type: "BatchNorm"
  bottom: "concat_4_35"
  top: "conv4_36/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_36/x1/scale"
  type: "Scale"
  bottom: "conv4_36/x1/bn"
  top: "conv4_36/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_36/x1"
  type: "ReLU"
  bottom: "conv4_36/x1/bn"
  top: "conv4_36/x1/bn"
}
layer {
  name: "conv4_36/x1"
  type: "Convolution"
  bottom: "conv4_36/x1/bn"
  top: "conv4_36/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv4_36/x2/bn"
  type: "BatchNorm"
  bottom: "conv4_36/x1"
  top: "conv4_36/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_36/x2/scale"
  type: "Scale"
  bottom: "conv4_36/x2/bn"
  top: "conv4_36/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_36/x2"
  type: "ReLU"
  bottom: "conv4_36/x2/bn"
  top: "conv4_36/x2/bn"
}
layer {
  name: "conv4_36/x2"
  type: "Convolution"
  bottom: "conv4_36/x2/bn"
  top: "conv4_36/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_4_36"
  type: "Concat"
  bottom: "concat_4_35"
  bottom: "conv4_36/x2"
  top: "concat_4_36"
}
layer {
  name: "conv4_blk/bn"
  type: "BatchNorm"
  bottom: "concat_4_36"
  top: "conv4_blk/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv4_blk/scale"
  type: "Scale"
  bottom: "conv4_blk/bn"
  top: "conv4_blk/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4_blk"
  type: "ReLU"
  bottom: "conv4_blk/bn"
  top: "conv4_blk/bn"
}
layer {
  name: "conv4_blk"
  type: "Convolution"
  bottom: "conv4_blk/bn"
  top: "conv4_blk"
  convolution_param {
    num_output: 1056
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "conv4_blk"
  top: "pool4"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv5_1/x1/bn"
  type: "BatchNorm"
  bottom: "pool4"
  top: "conv5_1/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_1/x1/scale"
  type: "Scale"
  bottom: "conv5_1/x1/bn"
  top: "conv5_1/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_1/x1"
  type: "ReLU"
  bottom: "conv5_1/x1/bn"
  top: "conv5_1/x1/bn"
}
layer {
  name: "conv5_1/x1"
  type: "Convolution"
  bottom: "conv5_1/x1/bn"
  top: "conv5_1/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_1/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_1/x1"
  top: "conv5_1/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_1/x2/scale"
  type: "Scale"
  bottom: "conv5_1/x2/bn"
  top: "conv5_1/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_1/x2"
  type: "ReLU"
  bottom: "conv5_1/x2/bn"
  top: "conv5_1/x2/bn"
}
layer {
  name: "conv5_1/x2"
  type: "Convolution"
  bottom: "conv5_1/x2/bn"
  top: "conv5_1/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_1"
  type: "Concat"
  bottom: "pool4"
  bottom: "conv5_1/x2"
  top: "concat_5_1"
}
layer {
  name: "conv5_2/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_1"
  top: "conv5_2/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_2/x1/scale"
  type: "Scale"
  bottom: "conv5_2/x1/bn"
  top: "conv5_2/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_2/x1"
  type: "ReLU"
  bottom: "conv5_2/x1/bn"
  top: "conv5_2/x1/bn"
}
layer {
  name: "conv5_2/x1"
  type: "Convolution"
  bottom: "conv5_2/x1/bn"
  top: "conv5_2/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_2/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_2/x1"
  top: "conv5_2/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_2/x2/scale"
  type: "Scale"
  bottom: "conv5_2/x2/bn"
  top: "conv5_2/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_2/x2"
  type: "ReLU"
  bottom: "conv5_2/x2/bn"
  top: "conv5_2/x2/bn"
}
layer {
  name: "conv5_2/x2"
  type: "Convolution"
  bottom: "conv5_2/x2/bn"
  top: "conv5_2/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_2"
  type: "Concat"
  bottom: "concat_5_1"
  bottom: "conv5_2/x2"
  top: "concat_5_2"
}
layer {
  name: "conv5_3/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_2"
  top: "conv5_3/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_3/x1/scale"
  type: "Scale"
  bottom: "conv5_3/x1/bn"
  top: "conv5_3/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_3/x1"
  type: "ReLU"
  bottom: "conv5_3/x1/bn"
  top: "conv5_3/x1/bn"
}
layer {
  name: "conv5_3/x1"
  type: "Convolution"
  bottom: "conv5_3/x1/bn"
  top: "conv5_3/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_3/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_3/x1"
  top: "conv5_3/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_3/x2/scale"
  type: "Scale"
  bottom: "conv5_3/x2/bn"
  top: "conv5_3/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_3/x2"
  type: "ReLU"
  bottom: "conv5_3/x2/bn"
  top: "conv5_3/x2/bn"
}
layer {
  name: "conv5_3/x2"
  type: "Convolution"
  bottom: "conv5_3/x2/bn"
  top: "conv5_3/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_3"
  type: "Concat"
  bottom: "concat_5_2"
  bottom: "conv5_3/x2"
  top: "concat_5_3"
}
layer {
  name: "conv5_4/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_3"
  top: "conv5_4/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_4/x1/scale"
  type: "Scale"
  bottom: "conv5_4/x1/bn"
  top: "conv5_4/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_4/x1"
  type: "ReLU"
  bottom: "conv5_4/x1/bn"
  top: "conv5_4/x1/bn"
}
layer {
  name: "conv5_4/x1"
  type: "Convolution"
  bottom: "conv5_4/x1/bn"
  top: "conv5_4/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_4/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_4/x1"
  top: "conv5_4/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_4/x2/scale"
  type: "Scale"
  bottom: "conv5_4/x2/bn"
  top: "conv5_4/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_4/x2"
  type: "ReLU"
  bottom: "conv5_4/x2/bn"
  top: "conv5_4/x2/bn"
}
layer {
  name: "conv5_4/x2"
  type: "Convolution"
  bottom: "conv5_4/x2/bn"
  top: "conv5_4/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_4"
  type: "Concat"
  bottom: "concat_5_3"
  bottom: "conv5_4/x2"
  top: "concat_5_4"
}
layer {
  name: "conv5_5/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_4"
  top: "conv5_5/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_5/x1/scale"
  type: "Scale"
  bottom: "conv5_5/x1/bn"
  top: "conv5_5/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_5/x1"
  type: "ReLU"
  bottom: "conv5_5/x1/bn"
  top: "conv5_5/x1/bn"
}
layer {
  name: "conv5_5/x1"
  type: "Convolution"
  bottom: "conv5_5/x1/bn"
  top: "conv5_5/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_5/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_5/x1"
  top: "conv5_5/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_5/x2/scale"
  type: "Scale"
  bottom: "conv5_5/x2/bn"
  top: "conv5_5/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_5/x2"
  type: "ReLU"
  bottom: "conv5_5/x2/bn"
  top: "conv5_5/x2/bn"
}
layer {
  name: "conv5_5/x2"
  type: "Convolution"
  bottom: "conv5_5/x2/bn"
  top: "conv5_5/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_5"
  type: "Concat"
  bottom: "concat_5_4"
  bottom: "conv5_5/x2"
  top: "concat_5_5"
}
layer {
  name: "conv5_6/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_5"
  top: "conv5_6/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_6/x1/scale"
  type: "Scale"
  bottom: "conv5_6/x1/bn"
  top: "conv5_6/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_6/x1"
  type: "ReLU"
  bottom: "conv5_6/x1/bn"
  top: "conv5_6/x1/bn"
}
layer {
  name: "conv5_6/x1"
  type: "Convolution"
  bottom: "conv5_6/x1/bn"
  top: "conv5_6/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_6/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_6/x1"
  top: "conv5_6/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_6/x2/scale"
  type: "Scale"
  bottom: "conv5_6/x2/bn"
  top: "conv5_6/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_6/x2"
  type: "ReLU"
  bottom: "conv5_6/x2/bn"
  top: "conv5_6/x2/bn"
}
layer {
  name: "conv5_6/x2"
  type: "Convolution"
  bottom: "conv5_6/x2/bn"
  top: "conv5_6/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_6"
  type: "Concat"
  bottom: "concat_5_5"
  bottom: "conv5_6/x2"
  top: "concat_5_6"
}
layer {
  name: "conv5_7/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_6"
  top: "conv5_7/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_7/x1/scale"
  type: "Scale"
  bottom: "conv5_7/x1/bn"
  top: "conv5_7/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_7/x1"
  type: "ReLU"
  bottom: "conv5_7/x1/bn"
  top: "conv5_7/x1/bn"
}
layer {
  name: "conv5_7/x1"
  type: "Convolution"
  bottom: "conv5_7/x1/bn"
  top: "conv5_7/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_7/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_7/x1"
  top: "conv5_7/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_7/x2/scale"
  type: "Scale"
  bottom: "conv5_7/x2/bn"
  top: "conv5_7/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_7/x2"
  type: "ReLU"
  bottom: "conv5_7/x2/bn"
  top: "conv5_7/x2/bn"
}
layer {
  name: "conv5_7/x2"
  type: "Convolution"
  bottom: "conv5_7/x2/bn"
  top: "conv5_7/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_7"
  type: "Concat"
  bottom: "concat_5_6"
  bottom: "conv5_7/x2"
  top: "concat_5_7"
}
layer {
  name: "conv5_8/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_7"
  top: "conv5_8/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_8/x1/scale"
  type: "Scale"
  bottom: "conv5_8/x1/bn"
  top: "conv5_8/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_8/x1"
  type: "ReLU"
  bottom: "conv5_8/x1/bn"
  top: "conv5_8/x1/bn"
}
layer {
  name: "conv5_8/x1"
  type: "Convolution"
  bottom: "conv5_8/x1/bn"
  top: "conv5_8/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_8/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_8/x1"
  top: "conv5_8/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_8/x2/scale"
  type: "Scale"
  bottom: "conv5_8/x2/bn"
  top: "conv5_8/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_8/x2"
  type: "ReLU"
  bottom: "conv5_8/x2/bn"
  top: "conv5_8/x2/bn"
}
layer {
  name: "conv5_8/x2"
  type: "Convolution"
  bottom: "conv5_8/x2/bn"
  top: "conv5_8/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_8"
  type: "Concat"
  bottom: "concat_5_7"
  bottom: "conv5_8/x2"
  top: "concat_5_8"
}
layer {
  name: "conv5_9/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_8"
  top: "conv5_9/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_9/x1/scale"
  type: "Scale"
  bottom: "conv5_9/x1/bn"
  top: "conv5_9/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_9/x1"
  type: "ReLU"
  bottom: "conv5_9/x1/bn"
  top: "conv5_9/x1/bn"
}
layer {
  name: "conv5_9/x1"
  type: "Convolution"
  bottom: "conv5_9/x1/bn"
  top: "conv5_9/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_9/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_9/x1"
  top: "conv5_9/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_9/x2/scale"
  type: "Scale"
  bottom: "conv5_9/x2/bn"
  top: "conv5_9/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_9/x2"
  type: "ReLU"
  bottom: "conv5_9/x2/bn"
  top: "conv5_9/x2/bn"
}
layer {
  name: "conv5_9/x2"
  type: "Convolution"
  bottom: "conv5_9/x2/bn"
  top: "conv5_9/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_9"
  type: "Concat"
  bottom: "concat_5_8"
  bottom: "conv5_9/x2"
  top: "concat_5_9"
}
layer {
  name: "conv5_10/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_9"
  top: "conv5_10/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_10/x1/scale"
  type: "Scale"
  bottom: "conv5_10/x1/bn"
  top: "conv5_10/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_10/x1"
  type: "ReLU"
  bottom: "conv5_10/x1/bn"
  top: "conv5_10/x1/bn"
}
layer {
  name: "conv5_10/x1"
  type: "Convolution"
  bottom: "conv5_10/x1/bn"
  top: "conv5_10/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_10/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_10/x1"
  top: "conv5_10/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_10/x2/scale"
  type: "Scale"
  bottom: "conv5_10/x2/bn"
  top: "conv5_10/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_10/x2"
  type: "ReLU"
  bottom: "conv5_10/x2/bn"
  top: "conv5_10/x2/bn"
}
layer {
  name: "conv5_10/x2"
  type: "Convolution"
  bottom: "conv5_10/x2/bn"
  top: "conv5_10/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_10"
  type: "Concat"
  bottom: "concat_5_9"
  bottom: "conv5_10/x2"
  top: "concat_5_10"
}
layer {
  name: "conv5_11/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_10"
  top: "conv5_11/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_11/x1/scale"
  type: "Scale"
  bottom: "conv5_11/x1/bn"
  top: "conv5_11/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_11/x1"
  type: "ReLU"
  bottom: "conv5_11/x1/bn"
  top: "conv5_11/x1/bn"
}
layer {
  name: "conv5_11/x1"
  type: "Convolution"
  bottom: "conv5_11/x1/bn"
  top: "conv5_11/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_11/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_11/x1"
  top: "conv5_11/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_11/x2/scale"
  type: "Scale"
  bottom: "conv5_11/x2/bn"
  top: "conv5_11/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_11/x2"
  type: "ReLU"
  bottom: "conv5_11/x2/bn"
  top: "conv5_11/x2/bn"
}
layer {
  name: "conv5_11/x2"
  type: "Convolution"
  bottom: "conv5_11/x2/bn"
  top: "conv5_11/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_11"
  type: "Concat"
  bottom: "concat_5_10"
  bottom: "conv5_11/x2"
  top: "concat_5_11"
}
layer {
  name: "conv5_12/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_11"
  top: "conv5_12/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_12/x1/scale"
  type: "Scale"
  bottom: "conv5_12/x1/bn"
  top: "conv5_12/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_12/x1"
  type: "ReLU"
  bottom: "conv5_12/x1/bn"
  top: "conv5_12/x1/bn"
}
layer {
  name: "conv5_12/x1"
  type: "Convolution"
  bottom: "conv5_12/x1/bn"
  top: "conv5_12/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_12/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_12/x1"
  top: "conv5_12/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_12/x2/scale"
  type: "Scale"
  bottom: "conv5_12/x2/bn"
  top: "conv5_12/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_12/x2"
  type: "ReLU"
  bottom: "conv5_12/x2/bn"
  top: "conv5_12/x2/bn"
}
layer {
  name: "conv5_12/x2"
  type: "Convolution"
  bottom: "conv5_12/x2/bn"
  top: "conv5_12/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_12"
  type: "Concat"
  bottom: "concat_5_11"
  bottom: "conv5_12/x2"
  top: "concat_5_12"
}
layer {
  name: "conv5_13/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_12"
  top: "conv5_13/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_13/x1/scale"
  type: "Scale"
  bottom: "conv5_13/x1/bn"
  top: "conv5_13/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_13/x1"
  type: "ReLU"
  bottom: "conv5_13/x1/bn"
  top: "conv5_13/x1/bn"
}
layer {
  name: "conv5_13/x1"
  type: "Convolution"
  bottom: "conv5_13/x1/bn"
  top: "conv5_13/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_13/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_13/x1"
  top: "conv5_13/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_13/x2/scale"
  type: "Scale"
  bottom: "conv5_13/x2/bn"
  top: "conv5_13/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_13/x2"
  type: "ReLU"
  bottom: "conv5_13/x2/bn"
  top: "conv5_13/x2/bn"
}
layer {
  name: "conv5_13/x2"
  type: "Convolution"
  bottom: "conv5_13/x2/bn"
  top: "conv5_13/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_13"
  type: "Concat"
  bottom: "concat_5_12"
  bottom: "conv5_13/x2"
  top: "concat_5_13"
}
layer {
  name: "conv5_14/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_13"
  top: "conv5_14/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_14/x1/scale"
  type: "Scale"
  bottom: "conv5_14/x1/bn"
  top: "conv5_14/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_14/x1"
  type: "ReLU"
  bottom: "conv5_14/x1/bn"
  top: "conv5_14/x1/bn"
}
layer {
  name: "conv5_14/x1"
  type: "Convolution"
  bottom: "conv5_14/x1/bn"
  top: "conv5_14/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_14/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_14/x1"
  top: "conv5_14/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_14/x2/scale"
  type: "Scale"
  bottom: "conv5_14/x2/bn"
  top: "conv5_14/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_14/x2"
  type: "ReLU"
  bottom: "conv5_14/x2/bn"
  top: "conv5_14/x2/bn"
}
layer {
  name: "conv5_14/x2"
  type: "Convolution"
  bottom: "conv5_14/x2/bn"
  top: "conv5_14/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_14"
  type: "Concat"
  bottom: "concat_5_13"
  bottom: "conv5_14/x2"
  top: "concat_5_14"
}
layer {
  name: "conv5_15/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_14"
  top: "conv5_15/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_15/x1/scale"
  type: "Scale"
  bottom: "conv5_15/x1/bn"
  top: "conv5_15/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_15/x1"
  type: "ReLU"
  bottom: "conv5_15/x1/bn"
  top: "conv5_15/x1/bn"
}
layer {
  name: "conv5_15/x1"
  type: "Convolution"
  bottom: "conv5_15/x1/bn"
  top: "conv5_15/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_15/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_15/x1"
  top: "conv5_15/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_15/x2/scale"
  type: "Scale"
  bottom: "conv5_15/x2/bn"
  top: "conv5_15/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_15/x2"
  type: "ReLU"
  bottom: "conv5_15/x2/bn"
  top: "conv5_15/x2/bn"
}
layer {
  name: "conv5_15/x2"
  type: "Convolution"
  bottom: "conv5_15/x2/bn"
  top: "conv5_15/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_15"
  type: "Concat"
  bottom: "concat_5_14"
  bottom: "conv5_15/x2"
  top: "concat_5_15"
}
layer {
  name: "conv5_16/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_15"
  top: "conv5_16/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_16/x1/scale"
  type: "Scale"
  bottom: "conv5_16/x1/bn"
  top: "conv5_16/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_16/x1"
  type: "ReLU"
  bottom: "conv5_16/x1/bn"
  top: "conv5_16/x1/bn"
}
layer {
  name: "conv5_16/x1"
  type: "Convolution"
  bottom: "conv5_16/x1/bn"
  top: "conv5_16/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_16/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_16/x1"
  top: "conv5_16/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_16/x2/scale"
  type: "Scale"
  bottom: "conv5_16/x2/bn"
  top: "conv5_16/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_16/x2"
  type: "ReLU"
  bottom: "conv5_16/x2/bn"
  top: "conv5_16/x2/bn"
}
layer {
  name: "conv5_16/x2"
  type: "Convolution"
  bottom: "conv5_16/x2/bn"
  top: "conv5_16/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_16"
  type: "Concat"
  bottom: "concat_5_15"
  bottom: "conv5_16/x2"
  top: "concat_5_16"
}
layer {
  name: "conv5_17/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_16"
  top: "conv5_17/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_17/x1/scale"
  type: "Scale"
  bottom: "conv5_17/x1/bn"
  top: "conv5_17/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_17/x1"
  type: "ReLU"
  bottom: "conv5_17/x1/bn"
  top: "conv5_17/x1/bn"
}
layer {
  name: "conv5_17/x1"
  type: "Convolution"
  bottom: "conv5_17/x1/bn"
  top: "conv5_17/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_17/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_17/x1"
  top: "conv5_17/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_17/x2/scale"
  type: "Scale"
  bottom: "conv5_17/x2/bn"
  top: "conv5_17/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_17/x2"
  type: "ReLU"
  bottom: "conv5_17/x2/bn"
  top: "conv5_17/x2/bn"
}
layer {
  name: "conv5_17/x2"
  type: "Convolution"
  bottom: "conv5_17/x2/bn"
  top: "conv5_17/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_17"
  type: "Concat"
  bottom: "concat_5_16"
  bottom: "conv5_17/x2"
  top: "concat_5_17"
}
layer {
  name: "conv5_18/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_17"
  top: "conv5_18/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_18/x1/scale"
  type: "Scale"
  bottom: "conv5_18/x1/bn"
  top: "conv5_18/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_18/x1"
  type: "ReLU"
  bottom: "conv5_18/x1/bn"
  top: "conv5_18/x1/bn"
}
layer {
  name: "conv5_18/x1"
  type: "Convolution"
  bottom: "conv5_18/x1/bn"
  top: "conv5_18/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_18/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_18/x1"
  top: "conv5_18/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_18/x2/scale"
  type: "Scale"
  bottom: "conv5_18/x2/bn"
  top: "conv5_18/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_18/x2"
  type: "ReLU"
  bottom: "conv5_18/x2/bn"
  top: "conv5_18/x2/bn"
}
layer {
  name: "conv5_18/x2"
  type: "Convolution"
  bottom: "conv5_18/x2/bn"
  top: "conv5_18/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_18"
  type: "Concat"
  bottom: "concat_5_17"
  bottom: "conv5_18/x2"
  top: "concat_5_18"
}
layer {
  name: "conv5_19/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_18"
  top: "conv5_19/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_19/x1/scale"
  type: "Scale"
  bottom: "conv5_19/x1/bn"
  top: "conv5_19/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_19/x1"
  type: "ReLU"
  bottom: "conv5_19/x1/bn"
  top: "conv5_19/x1/bn"
}
layer {
  name: "conv5_19/x1"
  type: "Convolution"
  bottom: "conv5_19/x1/bn"
  top: "conv5_19/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_19/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_19/x1"
  top: "conv5_19/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_19/x2/scale"
  type: "Scale"
  bottom: "conv5_19/x2/bn"
  top: "conv5_19/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_19/x2"
  type: "ReLU"
  bottom: "conv5_19/x2/bn"
  top: "conv5_19/x2/bn"
}
layer {
  name: "conv5_19/x2"
  type: "Convolution"
  bottom: "conv5_19/x2/bn"
  top: "conv5_19/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_19"
  type: "Concat"
  bottom: "concat_5_18"
  bottom: "conv5_19/x2"
  top: "concat_5_19"
}
layer {
  name: "conv5_20/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_19"
  top: "conv5_20/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_20/x1/scale"
  type: "Scale"
  bottom: "conv5_20/x1/bn"
  top: "conv5_20/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_20/x1"
  type: "ReLU"
  bottom: "conv5_20/x1/bn"
  top: "conv5_20/x1/bn"
}
layer {
  name: "conv5_20/x1"
  type: "Convolution"
  bottom: "conv5_20/x1/bn"
  top: "conv5_20/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_20/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_20/x1"
  top: "conv5_20/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_20/x2/scale"
  type: "Scale"
  bottom: "conv5_20/x2/bn"
  top: "conv5_20/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_20/x2"
  type: "ReLU"
  bottom: "conv5_20/x2/bn"
  top: "conv5_20/x2/bn"
}
layer {
  name: "conv5_20/x2"
  type: "Convolution"
  bottom: "conv5_20/x2/bn"
  top: "conv5_20/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_20"
  type: "Concat"
  bottom: "concat_5_19"
  bottom: "conv5_20/x2"
  top: "concat_5_20"
}
layer {
  name: "conv5_21/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_20"
  top: "conv5_21/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_21/x1/scale"
  type: "Scale"
  bottom: "conv5_21/x1/bn"
  top: "conv5_21/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_21/x1"
  type: "ReLU"
  bottom: "conv5_21/x1/bn"
  top: "conv5_21/x1/bn"
}
layer {
  name: "conv5_21/x1"
  type: "Convolution"
  bottom: "conv5_21/x1/bn"
  top: "conv5_21/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_21/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_21/x1"
  top: "conv5_21/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_21/x2/scale"
  type: "Scale"
  bottom: "conv5_21/x2/bn"
  top: "conv5_21/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_21/x2"
  type: "ReLU"
  bottom: "conv5_21/x2/bn"
  top: "conv5_21/x2/bn"
}
layer {
  name: "conv5_21/x2"
  type: "Convolution"
  bottom: "conv5_21/x2/bn"
  top: "conv5_21/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_21"
  type: "Concat"
  bottom: "concat_5_20"
  bottom: "conv5_21/x2"
  top: "concat_5_21"
}
layer {
  name: "conv5_22/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_21"
  top: "conv5_22/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_22/x1/scale"
  type: "Scale"
  bottom: "conv5_22/x1/bn"
  top: "conv5_22/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_22/x1"
  type: "ReLU"
  bottom: "conv5_22/x1/bn"
  top: "conv5_22/x1/bn"
}
layer {
  name: "conv5_22/x1"
  type: "Convolution"
  bottom: "conv5_22/x1/bn"
  top: "conv5_22/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_22/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_22/x1"
  top: "conv5_22/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_22/x2/scale"
  type: "Scale"
  bottom: "conv5_22/x2/bn"
  top: "conv5_22/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_22/x2"
  type: "ReLU"
  bottom: "conv5_22/x2/bn"
  top: "conv5_22/x2/bn"
}
layer {
  name: "conv5_22/x2"
  type: "Convolution"
  bottom: "conv5_22/x2/bn"
  top: "conv5_22/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_22"
  type: "Concat"
  bottom: "concat_5_21"
  bottom: "conv5_22/x2"
  top: "concat_5_22"
}
layer {
  name: "conv5_23/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_22"
  top: "conv5_23/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_23/x1/scale"
  type: "Scale"
  bottom: "conv5_23/x1/bn"
  top: "conv5_23/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_23/x1"
  type: "ReLU"
  bottom: "conv5_23/x1/bn"
  top: "conv5_23/x1/bn"
}
layer {
  name: "conv5_23/x1"
  type: "Convolution"
  bottom: "conv5_23/x1/bn"
  top: "conv5_23/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_23/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_23/x1"
  top: "conv5_23/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_23/x2/scale"
  type: "Scale"
  bottom: "conv5_23/x2/bn"
  top: "conv5_23/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_23/x2"
  type: "ReLU"
  bottom: "conv5_23/x2/bn"
  top: "conv5_23/x2/bn"
}
layer {
  name: "conv5_23/x2"
  type: "Convolution"
  bottom: "conv5_23/x2/bn"
  top: "conv5_23/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_23"
  type: "Concat"
  bottom: "concat_5_22"
  bottom: "conv5_23/x2"
  top: "concat_5_23"
}
layer {
  name: "conv5_24/x1/bn"
  type: "BatchNorm"
  bottom: "concat_5_23"
  top: "conv5_24/x1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_24/x1/scale"
  type: "Scale"
  bottom: "conv5_24/x1/bn"
  top: "conv5_24/x1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_24/x1"
  type: "ReLU"
  bottom: "conv5_24/x1/bn"
  top: "conv5_24/x1/bn"
}
layer {
  name: "conv5_24/x1"
  type: "Convolution"
  bottom: "conv5_24/x1/bn"
  top: "conv5_24/x1"
  convolution_param {
    num_output: 192
    bias_term: false
    kernel_size: 1
  }
}
layer {
  name: "conv5_24/x2/bn"
  type: "BatchNorm"
  bottom: "conv5_24/x1"
  top: "conv5_24/x2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_24/x2/scale"
  type: "Scale"
  bottom: "conv5_24/x2/bn"
  top: "conv5_24/x2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_24/x2"
  type: "ReLU"
  bottom: "conv5_24/x2/bn"
  top: "conv5_24/x2/bn"
}
layer {
  name: "conv5_24/x2"
  type: "Convolution"
  bottom: "conv5_24/x2/bn"
  top: "conv5_24/x2"
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "concat_5_24"
  type: "Concat"
  bottom: "concat_5_23"
  bottom: "conv5_24/x2"
  top: "concat_5_24"
}
layer {
  name: "conv5_blk/bn"
  type: "BatchNorm"
  bottom: "concat_5_24"
  top: "conv5_blk/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv5_blk/scale"
  type: "Scale"
  bottom: "conv5_blk/bn"
  top: "conv5_blk/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5_blk"
  type: "ReLU"
  bottom: "conv5_blk/bn"
  top: "conv5_blk/bn"
}
#upsample4started
#score shape: 1x1x10x10
#outpus shape: 20x20
#(20+2-4)/2+1=10
layer {
  name: "convd4_1"
  type: "Convolution"
  bottom: "conv5_blk/bn"
  top: "convd4_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 200
    kernel_size: 3
    pad: 16
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation:16
    engine: CAFFE
  }
}
layer {
  name: "deconv_4"
  type: "Deconvolution"
  bottom: "convd4_1"
  top: "outmap4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 200
    kernel_size: 4
    stride: 2
    pad:1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "P3O4"
  type: "Concat"
  bottom:"conv4_blk"
  bottom: "outmap4"
  top: "P3O4"
}
#upsample4end
#upsample3start
#score shape: 1x1x20x20
#outpus shape: 40x40
#(40+2-4)/2+1=20
layer {
  name: "convd4_2"
  type: "Convolution"
  bottom: "P3O4"
  top: "convd4_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 300
    kernel_size: 3
    pad: 8
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation:8
    engine: CAFFE
  }
}
layer {
  name: "deconv_3"
  type: "Deconvolution"
  bottom: "convd4_2"
  top: "outmap3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 300
    kernel_size: 4
    stride: 2
    pad:1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "P2O3"
  type: "Concat"
  bottom:"conv3_blk"
  bottom: "outmap3"
  top: "P2O3"
}
#upsample3end
#upsample2start
#score shape: 1x1x40x40
#outpus shape: 80x80
#(80+2-4)/2+1=40
layer {
  name: "convd4_3"
  type: "Convolution"
  bottom: "P2O3"
  top: "convd4_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 400
    kernel_size: 3
    pad: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation:4
    engine: CAFFE
  }
}
layer {
  name: "deconv_2"
  type: "Deconvolution"
  bottom: "convd4_3"
  top: "outmap2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 400
    kernel_size: 4
    stride: 2
    pad:1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "P1O2"
  type: "Concat"
  bottom:"conv2_blk"
  bottom: "outmap2"
  top: "P1O2"
}
#upsample2end
#upsample1start
#score shape: 1x1x80x80
#outpus shape: 160x160
#(160+2-4)/2+1=80
layer {
  name: "convd4_4"
  type: "Convolution"
  bottom: "P1O2"
  top: "convd4_4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 500
    kernel_size: 3
    pad: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation:2
    engine: CAFFE
  }
}
layer {
  name: "deconv_1"
  type: "Deconvolution"
  bottom: "convd4_4"
  top: "outmap1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 500
    kernel_size:4
    stride: 2
    pad:1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "outmap1/bn"
  type: "BatchNorm"
  bottom: "outmap1"
  top: "outmap1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "outmap1/scale"
  type: "Scale"
  bottom: "outmap1/bn"
  top: "outmap1/scale"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu6_1"
  type: "ReLU"
  bottom: "outmap1/scale"
  top: "outmap1/relu"
}
#upsample1end
#fifth denseblock
layer {
  name: "conv6_1"
  type: "Convolution"
  bottom: "outmap1/relu"
  top: "conv6_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 100
    kernel_size: 3
    pad: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation:2
    engine: CAFFE
  }
}
layer {
  name: "conv6_1/bn"
  type: "BatchNorm"
  bottom: "conv6_1"
  top: "conv6_1/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv6_1/scale"
  type: "Scale"
  bottom: "conv6_1/bn"
  top: "conv6_1/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu6_1"
  type: "ReLU"
  bottom: "conv6_1/bn"
  top: "conv6_1/bn"
}
layer {
  name: "conv6_2"
  type: "Convolution"
  bottom: "conv6_1/bn"
  top: "conv6_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 50
    kernel_size: 3
    pad: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation:4
    engine: CAFFE
  }
}
layer {
  name: "concat_6_1"
  type: "Concat"
  bottom: "conv6_1"
  bottom: "conv6_2"
  top: "concat_6_1"
}
layer {
  name: "conv6_2/bn"
  type: "BatchNorm"
  bottom: "concat_6_1"
  top: "conv6_2/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv6_2/scale"
  type: "Scale"
  bottom: "conv6_2/bn"
  top: "conv6_2/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu6_2"
  type: "ReLU"
  bottom: "conv6_2/bn"
  top: "conv6_2/bn"
}
layer {
  name: "conv6_3"
  type: "Convolution"
  bottom: "conv6_2/bn"
  top: "conv6_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 30
    kernel_size: 3
    pad: 8
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation:8
    engine: CAFFE
  }
}
layer {
  name: "concat_6_2"
  type: "Concat"
  bottom: "concat_6_1"
  bottom: "conv6_3"
  top: "concat_6_2"
}
layer {
  name: "conv6_3/bn"
  type: "BatchNorm"
  bottom: "concat_6_2"
  top: "conv6_3/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv6_3/scale"
  type: "Scale"
  bottom: "conv6_3/bn"
  top: "conv6_3/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu6_3"
  type: "ReLU"
  bottom: "conv6_3/bn"
  top: "conv6_3/bn"
}
layer {
  name: "conv6_4"
  type: "Convolution"
  bottom: "conv6_3/bn"
  top: "conv6_4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 20
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: CAFFE
  }
}
layer {
  name: "concat_6_3"
  type: "Concat"
  bottom: "concat_6_2"
  bottom: "conv6_4"
  top: "concat_6_3"
}
layer {
  name: "conv6_4/bn"
  type: "BatchNorm"
  bottom: "concat_6_3"
  top: "conv6_4/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv6_4/scale"
  type: "Scale"
  bottom: "conv6_4/bn"
  top: "conv6_4/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu6_4"
  type: "ReLU"
  bottom: "conv6_4/bn"
  top: "conv6_4/bn"
}
layer {
  name: "conv6_5"
  type: "Convolution"
  bottom: "conv6_4/bn"
  top: "conv6_5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    engine: CAFFE
  }
}
layer {
  name: "conv6_5/bn"
  type: "BatchNorm"
  bottom: "conv6_5"
  top: "conv6_5/bn"
  batch_norm_param {
    eps: 1e-5
  }
}
layer {
  name: "conv6_5/scale"
  type: "Scale"
  bottom: "conv6_5/bn"
  top: "conv6_5/bn"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu6_5"
  type: "ReLU"
  bottom: "conv6_5/bn"
  top: "conv6_5/bn"
}
#score shape: 1x1x160x160
#outpus shape: 320x320
#(320+2-4)/2+1=160
layer {
  name: "deconv_5"
  type: "Deconvolution"
  bottom: "conv6_5/bn"
  top: "deconv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1
    kernel_size: 4
    stride: 2
    pad:1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "sigmoid"
  type: "Sigmoid"
  bottom: "deconv5"
  top: "outmap"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "outmap"
  bottom: "inputmap"
  top: "loss"
}
